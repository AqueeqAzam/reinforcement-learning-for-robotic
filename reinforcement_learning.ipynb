{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNauvwgjyj8EnVsSox4YCXG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AqueeqAzam/reinforcement-learning-for-robotic/blob/main/reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Fundamental Concept`\n",
        "\n",
        "Agent: The actor performing whthin the env.\n",
        "\n",
        "Env: The region in which the actor can perform in.\n",
        "\n",
        "Action: The actor can take action within the env.\n",
        "\n",
        "Reward + Observation: Actor recevied a reward and a view of what the env look like after performing on it.\n",
        "\n",
        "Agent --> Env --> Action --> Reward + obs"
      ],
      "metadata": {
        "id": "5TNgZ0NQpXeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Setup dependencies`"
      ],
      "metadata": {
        "id": "ivhfEWiRt0gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "id": "_ZGcyUDopbS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# It provides functions for interacting with the operating system, such as creating directories, listing files and directories,pen_spark\n",
        "\n",
        "import gym\n",
        "# It  is a popular toolkit for developing and testing reinforcement learning (RL) environments.\n",
        "# It provides a standardized interface for creating and interacting with RL tasks.\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "# Importing the PPO class allows you to create an instance of a PPO agent to train in an RL environment.\n",
        "\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "# This function helps you evaluate the performance of an RL agent on a given environment.pen_spark\n",
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "# It can be beneficial for certain RL algorithms or training setups."
      ],
      "metadata": {
        "id": "cKGqHaHDvDAk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Load Env`"
      ],
      "metadata": {
        "id": "2W2ALI_xpXO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'CartPole-v1'\n",
        "\n",
        "#  the gym.make() function, which creates an instance of the environment you specified in env_name\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# display env name\n",
        "env_name\n",
        "\n",
        "# The reset() method returns the initial observation (state) of the environment, which the agent can use to make decisions.\n",
        "env.reset()\n",
        "\n",
        "# This method attempts to visually display the environment, which can be helpful for debugging or understanding the state of the environment.\n",
        "env.render()\n",
        "\n",
        "# The action_space defines the set of possible actions the agent can take (e.g., left force, right force),\n",
        "env.action_space.sample()\n",
        "\n",
        "# the observation_space defines the range of possible observations (states) the agent can receive from the environment (e.g., cart position, pole angle).\n",
        "env.observation_space.sample()\n",
        "\n",
        "# This line calls the close() method on the environment.\n",
        "env.close()\n",
        "\n",
        "env.step(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly00rYGqzIDE",
        "outputId": "d1921f14-7473-4e29-80fe-7fc6e2b0162b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.03690413,  0.20433587,  0.01245518, -0.2731477 ], dtype=float32),\n",
              " 1.0,\n",
              " False,\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Setup Env`"
      ],
      "metadata": {
        "id": "QMwhMmlfzvFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 5\n",
        "for episode in range(1, episodes+1):\n",
        "  print(episode)\n",
        "  state = env.reset()\n",
        "  print(state)\n",
        "\n",
        "  print(env.action_space.sample())\n",
        "  print(env.observation_space.sample())"
      ],
      "metadata": {
        "id": "SxSlHMR83jKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 5\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    while not done:\n",
        "        # env.render()\n",
        "        action = env.action_space.sample()\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))"
      ],
      "metadata": {
        "id": "Sxu576rnzyT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "metadata": {
        "id": "0Fw4XDw83eas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Understanding Env`"
      ],
      "metadata": {
        "id": "bCcyum6-8qmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space\n",
        "\n",
        "env.action_space.sample()\n",
        "\n",
        "env.observation_space\n",
        "env.observation_space.sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hT1-u9V8uIH",
        "outputId": "bcbb2fde-742b-49e7-c4a2-b17abdd19612"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.8230591e+00, -1.6741126e+38,  3.3211878e-01, -3.2379831e+38],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "HeMjAeLg94cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Simple Project`"
      ],
      "metadata": {
        "id": "3O_n_-MpSU-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "discount_factor = 0.95\n",
        "episodes = 1000\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Define the agent's policy network (model)\n",
        "model = Sequential([\n",
        "  Dense(16, activation='relu', input_shape=env.observation_space.shape),\n",
        "  Dense(2, activation='softmax')  # Output layer for action probabilities\n",
        "])\n",
        "\n",
        "# Define the loss function (categorical cross-entropy)\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Define the optimizer (Adam)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "def choose_action(state):\n",
        "  # Convert state to a tensor and add a batch dimension (for compatibility with model)\n",
        "  state_tensor = tf.expand_dims(state, axis=0)\n",
        "\n",
        "  # Predict action probabilities from the model\n",
        "  action_probs = model(state_tensor)\n",
        "\n",
        "  # Sample an action based on the probability distribution\n",
        "  action = tf.random.categorical(action_probs, 1)[0]  # Sample one action\n",
        "  return action.numpy()[0]  # Convert back to a NumPy value\n",
        "\n",
        "def train(state, action, reward, next_state, done):\n",
        "  # Convert state and next_state tensors to a batch dimension\n",
        "  state_tensor = tf.expand_dims(state, axis=0)\n",
        "  next_state_tensor = tf.expand_dims(next_state, axis=0)\n",
        "\n",
        "  # Get one-hot encoded action (binary representation of chosen action)\n",
        "  one_hot_action = tf.one_hot(action, env.action_space.n)\n",
        "\n",
        "  # Calculate the target value (expected future reward)\n",
        "  if done:\n",
        "    target_value = tf.constant([reward], dtype=tf.float32) # Convert reward to a tensor with shape (1,)\n",
        "  else:\n",
        "    # Use Bellman equation to calculate target value (consider discounted future reward)\n",
        "    q_values_next = model(next_state_tensor)\n",
        "    target_value = reward + discount_factor * tf.reduce_max(q_values_next, axis=1)\n",
        "\n",
        "  # Calculate the loss (difference between predicted Q-value and target value for the chosen action)\n",
        "  with tf.GradientTape() as tape:\n",
        "    q_values = model(state_tensor)\n",
        "    q_action = tf.reduce_sum(q_values * one_hot_action, axis=1)\n",
        "    loss = loss_fn(target_value, q_action)\n",
        "\n",
        "  # Calculate gradients with respect to model parameters\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Update model parameters using optimizer\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "# Training loop\n",
        "for episode in range(episodes):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  score = 0\n",
        "\n",
        "  while not done:\n",
        "    action = choose_action(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "\n",
        "    train(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "\n",
        "  print(f\"Episode: {episode+1}, Score: {score}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1lQdMrFjTbMa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}